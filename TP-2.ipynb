{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<center>\n",
    "    <img src=\"resource/logo_uvsq.jpg\" width=\"40%\" />\n",
    "    <br />\n",
    "    <h1>Machine Learning avec Scikit-Learn</h1>\n",
    "    <br /><br />\n",
    "    <a href=\"mailto:almada.livia@gmail.com\">Lívia Almada</a>\n",
    "    <br /><br />\n",
    "    Université de Versailles Saint-Quentain en Yveline\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Global settings\n",
    "import warnings\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "warnings.simplefilter(action=\"ignore\", category=UserWarning)\n",
    "warnings.simplefilter(action=\"ignore\", category=RuntimeWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Apprentissage non Supervisé (Unsupervised Learning)\n",
    "La différence entre ce type d'apprentissage et l'autre supervisé est que dans l'apprentissage non supervisé les données n'ont pas de classes (Labels). \n",
    "\n",
    "L'algorithm devrait trouver et construire des structures pratiques à partir des données non classifiées, i.e., Regrouper les données similaires. La similarité est déduite à partir des caractéristiques différentes dans les données.\n",
    "\n",
    "On appelle cette technique de regroupement: __Clustering__\n",
    "\n",
    "Il existe deux types de Clustering:\n",
    "1. Flat Clustering\n",
    "2. Hierarchical Clustering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Flat Clustering: K-Means\n",
    "K-Means est l'un des algorithmes de clustering les plus populaires. Il identifie K \"centroids\" qui sont utilisés pour définir les Clusters. Un point est considéré comme étant dans un cluster particulier s'il est plus proche du centoide de ce cluster que tout autre centroide.\n",
    "\n",
    "### Clustering Handwritten Digits\n",
    "On va voir comment K-means fonctionne en utilisant le chiffres dataset dans scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tutorial as tt\n",
    "\n",
    "# Une fonction pour visualiser les chiffres\n",
    "def print_digits(images, y, max_n=10):\n",
    "    plt.figure(1)\n",
    "    plt.clf()\n",
    "    fig = plt.figure(figsize=(12, 12))\n",
    "    fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05\n",
    "                        , wspace=0.05)\n",
    "    i = 0\n",
    "    while i < max_n and i<images.shape[0]:\n",
    "        p = fig.add_subplot(20, 20, i+1, xticks=[], yticks=[])\n",
    "        p.imshow(images[i], cmap=plt.cm.bone)\n",
    "        p.text(0, 14, str(y[i]))\n",
    "        i += 1\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Importer numpy\n",
    "import numpy as np\n",
    "\n",
    "# Importer load_digits de sklearn.datasets \n",
    "from sklearn.datasets import load_digits\n",
    "\n",
    "# Importer scale de sklearn.preprocessing\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "# Utiliser load_digits() pour charger les chiffres\n",
    "digits = load_digits()\n",
    "\n",
    "# Utiliser scale sur les données (data attribute)\n",
    "data = scale(digits.data)\n",
    "\n",
    "# Utiliser la fonction print_digits définie ci-dessus pour \n",
    " # visualiser les chiffres\n",
    "print_digits(digits.images, digits.target, max_n=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "On peut voir les chiffres (écrits à la main) avec les classes cibles au dessous. On ne va pas utiliser ces classes parce qu'on fait un apprentissage non suppervisé (Clustering).\n",
    "\n",
    "Ensuite, on va tester si on pourrait regroupper les figures en basant sur leur similarités. Normalement on devra avoir 10 clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Importer train_test_split de sklearn.model_selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Utiliser train_test_split pour diviser les données en deux parties. \n",
    "#  Une pour l'apprentissage et l'autre pour l'évaluation\n",
    "X_train, X_test, y_train, y_test, images_train, images_test = train_test_split(\n",
    "    digits.data, digits.target, digits.images, test_size=0.25,\n",
    "    random_state=41)\n",
    "\n",
    "# Récupérer le nombre d'échantillons et le nombre d'attributs (features)\n",
    "print(\"Nbr d'echan:\", digits.data.shape[0])\n",
    "print(\"Nbr d'attributs:\", digits.data.shape[1])\n",
    "\n",
    "# Utiliser la fonction unique dans numpy pour recupérer \n",
    "#  le nombre de chiffres\n",
    "unique_labels = np.unique(digits.target)\n",
    "print(unique_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Utiliser le KMeans dans sklearn.cluster pour \n",
    "#  initialiser le KMeans algorithm\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Il faut spécifier au moins deux paramètres pour KMeans:\n",
    "#  init et n_clusters\n",
    "clf = KMeans(init='k-means++', n_clusters=10)\n",
    "\n",
    "\n",
    "# Utiliser fit pour fair l'apprentissage\n",
    "clf.fit(X_train)\n",
    "\n",
    "# Quel est la différence entre la méthod \n",
    "#  fit ici et dans l'apprentissage supervisé?\n",
    "\n",
    "\n",
    "# Utiliser la fonction plot_clusters dans \n",
    "# tutorial.py pour visualiser le KMeans sur ces données\n",
    "tt.plot_clusters(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Visualiser les 10 premiers chiffres dans le dataset\n",
    "#  d'apprentissage avec les numéros de leurs cluster.\n",
    "# Utiliser l'attribut labels_ \n",
    "#  sur le KMeans instancié dans l'exercice ci-dessus\n",
    "print_digits(images_train, clf.labels_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Utiliser la méthode predict (comme dans l'apprentissage supervisé) \n",
    "#  pour fair le clustering du dataset d'évaluation\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Une fonction qui utilise le fancy indexing dans python pour\n",
    "#  sélectionner les images qui appartient dans un certain cluster\n",
    "def print_cluster(images, y_pred, cluster_number):\n",
    "    images = images[y_pred==cluster_number]\n",
    "    y_pred = y_pred[y_pred==cluster_number]\n",
    "    print_digits(images, y_pred, max_n=10)\n",
    "    \n",
    "# Utiliser la fonction print_cluster pour visualiser les dix\n",
    "#  premiers chiffres (dans le dataset d'évaluation) \n",
    "#  qui sont regroupés dans chaque cluster (de 0 à 10)\n",
    "for i in range(10):\n",
    "    print_cluster(images_test, y_pred, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Afficher la matrice de confusion dans sklearn.metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Clustering Iris Dataset\n",
    "On va utiliser K-Means pour le clustering du dataset des fleurs iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Comme dans load_digits, utiliser load_iris pour charger les données\n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "\n",
    "# Afficher la description du dataset iris\n",
    "print(iris.DESCR)\n",
    "# Récupérer le nombre d'échantillons et le nombre d'attributs (features)\n",
    "data = iris.data\n",
    "print(\"Nbr d'echan:\", data.shape[0])\n",
    "print(\"Nbr d'attributs:\", data.shape[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def plot_iris(X, y, x_axis, legend):\n",
    "    # X est un numpy array qui contient les échantillons\n",
    "    # y est un numpy array qui contient les classes \n",
    "    #  (le numéro du type de l'iris)\n",
    "    # x_axis est une liste qui contient les noms des attributs\n",
    "    #  (sepal length, sepal width,...)\n",
    "    # legend est une liste qui contient les noms des classes \n",
    "    #  (le type de l'iris)\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    colors = ['red', 'yellow', 'green']\n",
    "    for i in range(len(colors)):\n",
    "        start = True\n",
    "        for xs in X[y == i]:\n",
    "            if start:\n",
    "                ax.plot(xs,'o-', c=colors[i], label=legend[i])\n",
    "                start = False\n",
    "            else:\n",
    "                ax.plot(xs, 'o-', c=colors[i])\n",
    "    plt.xticks(range(len(x_axis)), x_axis, size='small')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def plot_iris_scatter(X, y, axis, legend, col1, col2):\n",
    "    # col1 et col2 sont des nombre entier entre 0 et 3 inclus, \n",
    "    #  pour sélectionner deux attributs à visualiser\n",
    "    colors = ['red', 'yellow', 'green']\n",
    "    for i in range(len(colors)):\n",
    "        xs = X[:, col1][y == i]\n",
    "        ys = X[:, col2][y == i]\n",
    "        plt.scatter(xs, ys, c = colors[i])\n",
    "    plt.legend(legend)\n",
    "    plt.xlabel(axis[col1])\n",
    "    plt.ylabel(axis[col2])\n",
    "    plt.show()\n",
    "\n",
    "# Utiliser plot_iris pour visualiser l'iris \n",
    "#  dataset avec tous le 4 attributs\n",
    "plot_iris(iris.data, iris.target, iris.feature_names, iris.target_names)\n",
    "\n",
    "# Utiliser plot_iris_scatter pour visualiser l'iris dataset \n",
    "#  avec juste 2 attributs. Il faut varier col1 et col2.\n",
    "plot_iris_scatter(iris.data, iris.target, iris.feature_names, \n",
    "                  iris.target_names,0, 1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Visualiser les iris en utilisant petal width et petal length\n",
    "#  (attribut numéro 2 et 3)\n",
    "plot_iris_scatter(iris.data, iris.target, iris.feature_names, \n",
    "                  iris.target_names,2, 3)\n",
    "\n",
    "# Utiliser train_test_split pour diviser les données en deux parties.\n",
    "#  Une pour l'apprentissage et l'autre pour l'évaluation\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target,\n",
    "                                                   test_size=0.25, \n",
    "                                                    random_state=40)\n",
    "\n",
    "# Utiliser K-Means pour faire le clustering. Faire l'apprentissage\n",
    "#  juste avec le petal width et petal length au début\n",
    "X = X_train[:,2:]\n",
    "kmeans = KMeans(init='k-means++', n_clusters=3)\n",
    "kmeans.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def plot_iris_clusters(clf, X, y, axis, legend, loc='lower right'):\n",
    "    h = .01\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), \n",
    "                         np.arange(y_min, y_max, h))\n",
    "    a = np.c_[xx.ravel(), yy.ravel()]\n",
    "    z = clf.predict(a)\n",
    "    z = z.reshape(xx.shape)\n",
    "    plt.imshow(z, interpolation='nearest', \n",
    "               extent=(xx.min(), xx.max(), yy.min(), yy.max()),\n",
    "               cmap=plt.cm.Paired, aspect='auto', origin='lower')\n",
    "    colors = ['red', 'yellow', 'green']\n",
    "    for i in range(len(colors)):\n",
    "        xs = X[:, 0][y == i]\n",
    "        ys = X[:, 1][y == i]\n",
    "        plt.scatter(xs, ys, c = colors[i])\n",
    "    plt.legend(legend, loc=loc)\n",
    "    plt.xlabel(axis[0])\n",
    "    plt.ylabel(axis[1])\n",
    "\n",
    "    centroids = clf.cluster_centers_\n",
    "    plt.scatter(centroids[:, 0], centroids[:, 1], marker='.',\n",
    "                s=169, linewidths=3, color='w', zorder=10)\n",
    "    plt.xlim(x_min, x_max)\n",
    "    plt.ylim(y_min, y_max)\n",
    "    plt.xticks(())\n",
    "    plt.yticks(())\n",
    "    plt.show()\n",
    "    \n",
    "# Utiliser la fonction plot_iris_clusters avec les données\n",
    "#  d'apprentissage pour voir les clusters\n",
    "plot_iris_clusters(kmeans, X, y_train, iris.feature_names[2:],\n",
    "                  iris.target_names)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Utiliser la fonction plot_iris_clusters avec \n",
    "#  les données d'évaluation pour voir les clusters\n",
    "X_eval = X_test[:,2:]\n",
    "plot_iris_clusters(kmeans, X_eval, y_test, iris.feature_names[2:],\n",
    "                  iris.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Répéter les mêmes étapes (visualisation et apprentissage)\n",
    "#  mais en utilisant sepal width et sepal length (0 et 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Et puis répéter une autre fois avec sepal width et petal length (1 et 2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Démo: DBSCAN\n",
    "\n",
    "On va essayer l'algorithm DBSCAN dans sckit-learn avec des données synthétiques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "centers = [[1, 1], [-1, -1], [1, -1]]\n",
    "x, labels_true = make_blobs(n_samples=750, centers=centers, \n",
    "                            cluster_std=0.4,\n",
    "                            random_state=0)\n",
    "plt.scatter(x[:,0], x[:,1])\n",
    "plt.show()\n",
    "\n",
    "db = DBSCAN(eps=0.3, min_samples=10).fit(x)\n",
    "labels = set(db.labels_)\n",
    "n_clusters = len(labels) - (1 if -1 in labels else 0)\n",
    "print(\"Nombre de clusters:\", n_clusters)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def plot_db_clusters(db, X):\n",
    "    core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n",
    "    core_samples_mask[db.core_sample_indices_] = True\n",
    "    unique_labels = set(db.labels_)\n",
    "    n_clusters = len(unique_labels) - (1 if -1 in labels else 0)\n",
    "    colors = plt.cm.Spectral(np.linspace(0, 1, len(unique_labels)))\n",
    "    for k, col in zip(unique_labels, colors):\n",
    "        if k == -1:\n",
    "            # Black used for noise.\n",
    "            col = 'k'\n",
    "\n",
    "        class_member_mask = (db.labels_ == k)\n",
    "\n",
    "        xy = X[class_member_mask & core_samples_mask]\n",
    "        plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=col,\n",
    "                 markeredgecolor='k', markersize=14)\n",
    "\n",
    "        xy = X[class_member_mask & ~core_samples_mask]\n",
    "        plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=col,\n",
    "                 markeredgecolor='k', markersize=6)\n",
    "\n",
    "    plt.title('Nombre de Clusters: %d' % n_clusters)\n",
    "    plt.show()\n",
    "\n",
    "plot_db_clusters(db, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Créer des autre dataset synthétiques avec make_blobs\n",
    "#  varier les differents paramètres \n",
    "# Et experimenter avec l'algorithm DBSCAN\n",
    "centers = [[1, 1], [-1, -1], [1, -1], [2,3], [-2,-4]]\n",
    "x, labels_true = make_blobs(n_samples=750, centers=centers, \n",
    "                            cluster_std=0.4,\n",
    "                            random_state=0)\n",
    "plt.scatter(x[:,0], x[:,1])\n",
    "plt.show()\n",
    "\n",
    "db = DBSCAN(eps=0.3, min_samples=10).fit(x)\n",
    "plot_db_clusters(db, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Essayer DBSCAN avec petal width et petal length de l'iris\n",
    "x = iris.data[:,2:]\n",
    "\n",
    "plot_iris_scatter(iris.data, iris.target, iris.feature_names, \n",
    "                  iris.target_names,2, 3)\n",
    "\n",
    "db = DBSCAN(eps=0.3, min_samples=10).fit(x)\n",
    "plot_db_clusters(db, x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
